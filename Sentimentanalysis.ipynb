{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimentanalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZdAllWAVFTS"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_MsHOAIEZN"
      },
      "source": [
        "imdb_data=pd.read_csv('/content/IMDB Dataset.csv')\n",
        "imdb_data=imdb_data[:5000]\n",
        "imdb_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFORFroLVlXo",
        "outputId": "2e907066-2077-4b45-d722-01f162d64ad9"
      },
      "source": [
        "d={\"sentiment\":{\"positive\":1,\"negative\":0}}\n",
        "imdb_data=imdb_data.replace(d)\n",
        "print(imdb_data[\"sentiment\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       0\n",
            "4       1\n",
            "       ..\n",
            "4995    0\n",
            "4996    1\n",
            "4997    1\n",
            "4998    0\n",
            "4999    0\n",
            "Name: sentiment, Length: 5000, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ZBkxQ1YmcX",
        "outputId": "792d6407-5852-473d-9da4-cb1f5faeeee0"
      },
      "source": [
        "imdb_data['sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2532\n",
              "1    2468\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bMEWLQ3le4e"
      },
      "source": [
        "###Preprocessing of text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku-skRVOaiAZ"
      },
      "source": [
        "import re\n",
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    return re.sub('(<br\\s*/><br\\s*/>)|(\\-)|(\\/)', '', text)\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('[.;:!\\'?,\\\"()\\[\\]]', '', text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukasRTdGc0Yp"
      },
      "source": [
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwnJjTP9q8dO",
        "outputId": "911ff9e1-49a1-41a5-9e29-5ccd5676afb0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove(text):\n",
        "  word_tokens = word_tokenize(text) \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "  filtered_sentence = []  \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  filter=' '.join(filtered_sentence)\n",
        "  return filter\n",
        "imdb_data['review']=imdb_data['review'].apply(remove)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gma7_gDptQSc",
        "outputId": "f08f1ae3-fa01-4534-c0a7-dccf36736e4d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "def lemma(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_tokens])\n",
        "  return lemmatized_output\n",
        "imdb_data['review']=imdb_data['review'].apply(lemma)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q_ACgt0w-u5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=imdb_data['review']\n",
        "Y=imdb_data['sentiment']\n",
        "X1=imdb_data['review']\n",
        "Y1=imdb_data['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.40, random_state=42)\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.40, random_state=42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acbhcll1yoXp"
      },
      "source": [
        "X_train=imdb_data.review[:3000]\n",
        "X_test=imdb_data.review[3000:]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "vectorizer1 = CountVectorizer(binary=False, ngram_range=(1, 3), stop_words=stop_words)\n",
        "cv_train =vectorizer1.fit_transform(X_train)\n",
        "cv_test=vectorizer1.transform(X_test)\n",
        "vectorizer2 =TfidfVectorizer(stop_words=stop_words)\n",
        "tf_train=vectorizer2.fit_transform(X_train)\n",
        "tf_test=vectorizer2.transform(X_test)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlAXoQap9TGs"
      },
      "source": [
        "Y_train=imdb_data.sentiment[:3000]\n",
        "Y_test=imdb_data.sentiment[3000:]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cttE0-7h8tmJ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZu6m63j9_ln"
      },
      "source": [
        "log = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "nvb = MultinomialNB()\n",
        "voting = VotingClassifier(estimators=[('lr', log), ('rf', rnd),('nv',nvb)],voting='hard')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkO19Ukzgbew"
      },
      "source": [
        "###Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXPyE6CWAHbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd5b514-a1cc-433b-8cd1-71c3e3433297"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "model1=log.fit(tf_train, Y_train)\n",
        "y_pred = model1.predict(tf_test)\n",
        "print(\"logistic regression for tf-idf\", accuracy_score(Y_test, y_pred))\n",
        "model11=log.fit(cv_train, Y_train)\n",
        "y_pred = model1.predict(cv_test)\n",
        "print(\"logistic regression for count vectorizer\", accuracy_score(Y_test, y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression for tf-idf 0.856\n",
            "logistic regression for count vectorizer 0.8495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DBGxiTyhRZ2"
      },
      "source": [
        "###Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDrB25npdwct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18b049e-7743-4634-8414-b676ce917e57"
      },
      "source": [
        "model2=nvb.fit(tf_train, Y_train)\n",
        "y_pred = model3.predict(tf_test)\n",
        "print(\"naive bayes for tf-idf\", accuracy_score(Y_test, y_pred))\n",
        "model21=nvb.fit(cv_train, Y_train)\n",
        "y_pred = model3.predict(cv_test)\n",
        "print(\"naive bayes for count vectorizer\", accuracy_score(Y_test, y_pred))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive bayes for tf-idf 0.846\n",
            "naive bayes for count vectorizer 0.8575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59TncnLkhYak"
      },
      "source": [
        "###Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9N_Q1IgdRYE",
        "outputId": "9988aa1c-4c93-4196-819f-de23a74eabf2"
      },
      "source": [
        "model61=rnd.fit(tf_train, Y_train)\n",
        "y_pred = model61.predict(tf_test)\n",
        "print(\"random forest for tf-idf\", accuracy_score(Y_test, y_pred))\n",
        "model6=rnd.fit(cv_train, Y_train)\n",
        "y_pred = model6.predict(cv_test)\n",
        "print(\"random forest for count vectorizer\", accuracy_score(Y_test, y_pred))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest for tf-idf 0.826\n",
            "random forest for count vectorizer 0.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQEuPuxdhuhK"
      },
      "source": [
        "###LinearSVC for tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff_2RoIATtQH",
        "outputId": "5fdd42e4-7288-43b5-9d69-2faa67d77704"
      },
      "source": [
        "regularization = [0.01, 0.05, 0.25, 0.5, 1]\n",
        "accuracy=[]\n",
        "for c in regularization:\n",
        "    lr = LinearSVC(C=c)\n",
        "    lr.fit(tf_train, Y_train)\n",
        "    accuracy.append(accuracy_score(Y_test, lr.predict(tf_test)))\n",
        "    print(\"Accuracy for C=%s: %s\" % (c, accuracy_score(Y_test, lr.predict(tf_test))))\n",
        "# train final model with highest c\n",
        "highestC = [regularization[i] for i in range(len(regularization)) if accuracy[i] == max(accuracy)][0]\n",
        "final_model1 = LinearSVC(C = highestC)\n",
        "final_model1.fit(tf_train, Y_train)\n",
        "y_pred = final_model1.predict(tf_test)\n",
        "print(\"Final Accuracy: %s\" % accuracy_score(Y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.807\n",
            "Accuracy for C=0.05: 0.8455\n",
            "Accuracy for C=0.25: 0.8615\n",
            "Accuracy for C=0.5: 0.8635\n",
            "Accuracy for C=1: 0.8595\n",
            "Final Accuracy: 0.8635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmQycIMbh4JA"
      },
      "source": [
        "###LinearSVC for count vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzO_9R7FiGGc",
        "outputId": "ff05c8c1-58eb-45ef-ebeb-8b21b622a7b2"
      },
      "source": [
        "regularization = [0.01, 0.05, 0.25, 0.5, 1]\n",
        "accuracy=[]\n",
        "for c in regularization:\n",
        "    lr = LinearSVC(C=c)\n",
        "    lr.fit(cv_train, Y_train)\n",
        "    accuracy.append(accuracy_score(Y_test, lr.predict(cv_test)))\n",
        "    print(\"Accuracy for C=%s: %s\" % (c, accuracy_score(Y_test, lr.predict(cv_test))))\n",
        "# train final model with highest c\n",
        "highestC = [regularization[i] for i in range(len(regularization)) if accuracy[i] == max(accuracy)][0]\n",
        "final_model11 = LinearSVC(C = highestC)\n",
        "final_model11.fit(cv_train, Y_train)\n",
        "y_pred = final_model11.predict(cv_test)\n",
        "print(\"Final Accuracy: %s\" % accuracy_score(Y_test, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.8475\n",
            "Accuracy for C=0.05: 0.852\n",
            "Accuracy for C=0.25: 0.852\n",
            "Accuracy for C=0.5: 0.8525\n",
            "Accuracy for C=1: 0.8515\n",
            "Final Accuracy: 0.8525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU_Q-NM1ilcN"
      },
      "source": [
        "###Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjBvRu4dU0Wi",
        "outputId": "45f088b7-28c0-4b21-b1ac-597023e0b5e8"
      },
      "source": [
        "voting_clf=VotingClassifier(estimators=[('lr',model11),('rf',model6),('svc',final_model11),('nvb',model21)],voting='hard')\n",
        "vote=voting_clf.fit(cv_train,Y_train)\n",
        "y_pred=vote.predict(cv_test)\n",
        "print(\"voting classifier for count vectorizer\", accuracy_score(Y_test, y_pred))\n",
        "voting_clf=VotingClassifier(estimators=[('lr',model1),('rf',model61),('svc',final_model1),('nvb',model2)],voting='hard')\n",
        "vote1=voting_clf.fit(tf_train,Y_train)\n",
        "y_pred1=vote1.predict(tf_test)\n",
        "print(\"voting classifier for tf-idf vectorizer\", accuracy_score(Y_test, y_pred1))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "voting classifier for count vectorizer 0.86\n",
            "voting classifier for tf-idf vectorizer 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1VgFOlUeBRj"
      },
      "source": [
        "import pickle\n",
        "pickle_out = open(\"vote.pkl\",\"wb\")\n",
        "pickle.dump(vote, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}